\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{float}
\usepackage{todonotes}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\arrowright}{$\,\to\,$}

\DeclareMathOperator{\atantwo}{atan2}
    
\begin{document}

\title{App-controlled LEGO robotic arm\\
{\footnotesize Project of lecture ''Mobile Computing'' (winter term 2018/2019)}
}

\author{
\IEEEauthorblockN{Christoph Ulrich}
\IEEEauthorblockA{%\textit{dept. name of organization (of Aff.)} \\
\textit{HTWG Konstanz}\\
Constance, Germany\\
christoph.ulrich@htwg-konstanz.de}
\and
\IEEEauthorblockN{Benjamin Schaefer}
\IEEEauthorblockA{%\textit{dept. name of organization (of Aff.)} \\
\textit{HTWG Konstanz}\\
Constance, Germany\\
benjamin.schaefer@htwg-konstanz.de}
}

\maketitle

\begin{abstract}
In this work, a method for building and controlling a robot arm using an Android APP is proposed. The Android phone and the NXT board are connected via WLAN, through an additional computer. From the Android app commands can be generated via buttons or a graphical interface, which are received by the NXT and the robot arm controls. The 'servers
-side' code is written in Python, the 'client-side' code is written in Java for Android. As usual in robotics, the ROS framework is used for communication.
\end{abstract}

%\begin{IEEEkeywords}
%component, formatting, style, styling, insert
%\end{IEEEkeywords}\

\section{Introduction / Motivation}
Control of a robotic arm is a fundamental task in robotics. With this theoretical work and practical implementation we want to give an easy hands-on experience to everyone who wants to start with robotics.\\
\par
The application and hardware should fulfill a quality which allows it to be used as example in basic robotic lectures at the HTWG Konstanz to give students a clear insight in robot kinematics and the established robotic framework \textit{Robot Operating System}\cite{onlROS}, which is also taught in lectures at the HTWG - everything combined in a portable format. 
Another goal was to recycle old and unused hardware of the robotics lab at the HTWG Konstanz and to try to use as few newly bought parts as possible.
\\
\par
Constructing a robotic arm and developing a mobile application for robotics generally lead to some difficulties.
For the mechanical/electrical part there are often a lot of parts which have to be assembled in a somewhat complex way - if you don't want to buy new stuff and if you want to achieve a satisfying accuracy of the arm. The system should furthermore be able to drive the arm even with more than one joint and with gripper load (to demonstrate basic gripping).
Typical industrial applications run on more powerful hardware and often offer a complicated and - for beginners - confusing user interface. One also has to deal with the loss of control commands due to radio lacks (if controlled via WiFi or Bluetooth). So an easy to use mobile application for Android platforms has to be developed. 

\section{State of the Art}
Although current approaches such as in \cite{onlIEEEApp-Arm} already control robotic arms using an app, but here usually an Arduino or a Raspberry Pi is used to control the arm. However, no hardware that can be reused from the lab is used here, and no known robotic frameworks are used here, which makes an extension or a rebuild more sophisticated. Arms made from Lego Technic as in \cite{onlNXT-2DOF} have already been built, but these only offer two Degrees of Freedom (\textit{DoF}) and can only lift very light objects due to the low gear ratio. However, in order to be able to show kinematics and the problems with robotic arms in teaching, at least three DoF are necessary.
Also some private projects like in \cite{onlHackster} show how to control a homemade arm via an app.
\section{Proposed Approach}\label{sec:approach}

\subsection{Requirements Engineering}\label{sec:requirements}
The app should have the ability to control the arm via a graphical interface. To control the joints individually, there should also be a possibility to adjust the individual joints via buttons or sliders. Since the servomotors are likely to drift away there should be a way to re-calibrate the arm at any time.
\\\\
The arm should be completely controllable by means of an mobile app via ROS. As the robotics lab at the HTWG has a lot of LEGO Technic remainders, the goal is to build the arm completely from these parts. In addition, the arm should be kept as light as possible in order to lift light 3D printed objects. It should also have a comfortable size and consume as little energy as possible so that the arm can be demonstrated in the teaching. 

\subsection{Platform Decision}\label{sec:platform}
Since the arm and the app should be used for teaching, there was a variety of hardware to choose from. In addition, one goal was to use existing and disused hardware. Thus, the decision quickly fell on the NXT bricks of Lego Mindstorm \cite{onlLegoMindstorm}. There are enough bricks as well as sensors and motors in the lab. In addition, from the existing LEGO technology also the arm can be constructed. Since LEGO Technic is made of lightweight plastic, the finished arm can also be taken to a lecture and presented.\\
There was also a choice of Raspberry Pi's or Arduinos, but here it would have been necessary to order hardware such as motors or parts for the arm.\\
Thus, it offered itself to use the NXT bricks, since no hardware needs to be ordered here and disused hardware is used again. For basic communication and underlying framework we decided for ROS. It is widely used in the field of robotics, very modular/extensible and almost every student starting with robotics has to get in touch with it.

\subsection{Arm Construction}\label{sec:construction}
\begin{figure}[bt] 
	\centering
	\includegraphics[width=\textwidth/2]{img/arm_full}
	\caption[caption]{Side view of the LEGO Technic arm. The base is marked in red, the lower joint in blue and the upper joint in green.}
	\label{fig:arm_full}
\end{figure}
The arm, which can be seen in \autoref{fig:arm_full}, was designed entirely from LEGO Technic. For this purpose, two NXT bricks were used. In addition, four motors and a touch sensor were used. One NXT was used to control the two arm joints, as well as the gripper. Since the NXT bricks only have three ports for motors, a second NXT was needed to rotate the base.\\
The servomotors have a built-in rotation sensor with an accuracy of one degree. Since these servomotors only have a torque of about 12 N.cm, translations has been built behind the joints so that the motors can move the arm. For this purpose, a translation of 1/42 was used for the lower joint. The engine's ratio at the upper joint is 2/25.\\
In order to keep the arm as light as possible and to relieve the engines, sensors on the arm itself were completely dispensed with. However, since the engines have to be initialized, a touch sensor has been installed on the base to initialize the first arm articulated. The initialization of the second joint was solved by the torque of the motors, as there is less weight of the arm.

\subsection{Algorithms}\label{sec:algorithms}
\subsubsection{Calibration}\label{sec:calibration}
\begin{figure}[htbp] 
	\centering
	\includegraphics[width=\textwidth/2]{img/calibration.png}
	\caption[caption]{Activity diagram of the calibration process.}
\label{fig:calibration}
\end{figure}
The following describes the calibration procedure, which is shown in \autoref{fig:calibration}. Since the servomotors can only be controlled by the effort and have no absolute encoder, the starting positions of the motors are unknown. Therefore, these must be calibrated. A touch sensor is used for the lower joint. The lower motor rotates until the touch sensor responds to a touch. Then the upper arm is calibrated. Here, the engines rotate with the least possible force until the joint is in the final position and the engines cannot turn any more, because the effort is too great. In order not to burden the motors unnecessarily, the calibration process is terminated as soon as a certain threshold has been exceeded. Finally, the same procedure is repeated for the gripper.

\subsubsection{Forward Kinematics}\label{sec:forwardkinematics}
In order to control the arm it is essential to solve either it's forward or inverse kinematics. For our application, we need both problems to be solved. Users of the application should be able to directly control each joint individually (forward kinematics) as well as to move the Tool Center Point (\textit{TCP}) into a desired position (inverse kinematics). We will first take a look at the calculation of the forward kinematics and then introduce a short geometric solution of the inverse kinematics for our 3-DoF arm.

\par
The goal of forward kinematics is to determine the pose (position and orientation) of the TCP for a given set of joint angles. The pose of the TCP regarding the origin of the robot arm can be described as a concatenation of $n$ transformations, where $n$ is the number of joints, of which every one has its own coordinate system. For our case $n$ is 3:
\begin{equation}\label{eq:trans_matrix}
T_0^3 = T_0^1 * T_1^2 * T_2^3
\end{equation}
where $T_i^{i-1}$ is a transformation matrix according to the \textit{Denavit-Hartenberg} convention:
\begin{equation}\label{eq:dh-convention}
T_i^{i-1} = Tl(0,0,d_i) * R(z, \theta_i) * Tl(a_i,0,0) * R(x, \alpha_i)
\end{equation}
with $Tl$ being a translation matrix, $R$ a rotation matrix, $d_i$ a translation in z, $a_i$ a translation (for prismatic joints) in x, $\theta_i$ a rotation around z and $\alpha_i$ a constant tilt angle between both joints. In our case we don't use prismatic joints and there are no constant tilt angles between the joints (they are all variable). So \autoref{eq:dh-convention} simplifies to 
\begin{equation}\label{eq:dh-convention-short}
T_i^{i-1} = Tl(0,0,d_i) * R(z, \theta_i)
\end{equation}
By solving \autoref{eq:dh-convention-short} we get the following transformation matrix $T_i^{i-1}$ for the forward kinematics:
\[
\begin{matrix}
\cos(\theta_i) & -\sin(\theta_i) & 0 & l_i \cdot \cos(\theta_i)  \\
\sin(\theta_i) & \cos(\theta_i) & 0 & l_i \cdot \sin(\theta_i) \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{matrix}
\]
Finally, one generally can get the pose of the TCP in the world coordinate system (usually that means with reference to the coordinate system of the robot's base) by multiplying the concatenated transformation matrix \ref{eq:trans_matrix} built from the set of given joint angles $\theta_i$ and the known lengths of the arm parts $l_i$ with the origin of the base:
\begin{equation}\label{eq:tcp_goal}
p_{tcp} = T_0^3 * \begin{pmatrix}0\\0\\0\\1\end{pmatrix}
\end{equation}
Note that one has to use homogeneous coordinates here. Also one should remember that $p_{tcp}$ is the pose of the TCP-origin. Usually users want to get the gripping point of their tool - in this case just add the half of the gripper length to the x-direction to the vector in \autoref{eq:tcp_goal}.
\\
\par

\subsubsection{Inverse Kinematics}\label{sec:inversekinematics}
As we saw previously, forward kinematics is a function $f$ with reference to a set of joint angles $\theta_i$. The goal of inverse kinematics is to calculate a set of joint angles from a desired TCP-pose. So obviously what we are searching for is the inverse of $f$:

\begin{equation}
f^{-1}(\theta_0, \theta_1, ..., \theta_n)
\end{equation}
There are several possible approaches to this problem - it can be solved either by algebraic, numerical or geometrical methods. We will briefly explain a geometrical solution.

We have to find the rotation angles for the three joints given a specific pose of the end-effector.
\autoref{fig:yaw_calc} shows a schematic top view of the robot:

\begin{figure}[htbp]
	\centerline{\includegraphics[scale=0.3]{img/kin_yaw_arm_top_view.png}}
	\caption{Schemactic top view of the robot. The two arm parts are coloured. We are searching for rotation angle $\theta$ of the base joint.}
	\label{fig:yaw_calc}
\end{figure}

As one can see, $\theta_1$ can easily be calculated:
\begin{equation}
\theta_1 = \atantwo(y,x)
\end{equation}

\autoref{fig:q1_q2_calc} shows the schematic side view of the robot.

\begin{figure}[htbp]
	\centerline{\includegraphics[scale=0.4]{img/kin_q1_q2.png}}
	\caption{Schematic side view of the robot. Joint angles $q_1$ and $q_2$ can be calculated with the support angles $\alpha$, $\beta$ and $\chi$.}
	\label{fig:q1_q2_calc}
\end{figure}

First of all, $\alpha$ can be calculated by
\begin{equation}
\alpha = \atantwo(p_y, p_x)
\end{equation}

Then, the law of cosines is used to calculate $\beta$ and $\chi$.
\begin{equation}
\l_2^2 = |\textbf{p}|^2 - 2 \cdot l_1 \cdot |\textbf{p}| \cdot \cos(\beta)
\end{equation}
with $l_2$ being the length of arm 2. From this follows

\begin{equation}
\beta = \arccos(\frac{\l_1^2 + |\textbf{p}|^2 - l_2^2}{2 \cdot l_1 \cdot |\textbf{p}|})
\end{equation}

Furthermore

\begin{equation}
|\textbf{p}|^2 = l_2^2 + l_1^2 - 2 \cdot l_1 \cdot l_2 \cdot \cos(\chi)
\end{equation}

with $l_1$ being the length of arm 1. From this follows

\begin{equation}
\chi = \arccos(\frac{\l_1 + l_2 - |\textbf{p}|^2}{2 \cdot l_1 \cdot l_2})
\end{equation}

With having calculated $\alpha$, $\beta$ and $\chi$, one can easily get $\theta_2$ and $\theta_3$ \rightarrow \ $\theta_2$ = $\alpha$ - $\beta$, $\theta_3$ = $\pi$ - $\chi$.


\subsection{App Architecture}\label{sec:architecture}
The app consists of two major activities and five major components, which will be described shortly in this paragraph. \autoref{fig:app_pipeline} shows the pipeline after a user has tapped on the screen to determine a TCP pose.

\begin{figure}[htbp]
	\centerline{\includegraphics[scale=0.15]{img/app_pipeline.png}}
	\caption{Control flow diagram of the main components of the app. Shows the pipeline after a user has tapped on the screen to determine a TCP pose.}
	\label{fig:app_pipeline}
\end{figure}

The blue components in \autoref{fig:app_pipeline} represent modules in our app, the labeled arrows between them describe the control/information flow.

\subsubsection{Joint Control Activity}
This activity offers buttons to calibrate the whole arm and to control each of the joints directly.

\subsubsection{TCP Control Activity}
This activity offers a GUI on which the robot arm is pictured (schematically). The user can tap on any point in a predefined operational area of the robot and thereby determining the next pose of the TCP. If the inverse kinematics module, whose theory was described in section \ref{sec:inversekinematics}, finds a valid configuration for this point, the angles are transmitted to the Action Client Module (\ref{sec:actionclient}).

\subsubsection{Forward Kinematics}
This module takes an angle configuration and calculates the resulting TCP pose as described in section \ref{sec:forwardkinematics}.

\subsubsection{Inverse Kinematics}
This module takes x- and y-coordinates and calculates a possible angle configuration as described in section \ref{sec:inversekinematics}.

\subsubsection{ROS Action Server}\label{sec:actionserver}
The Action Server is built with the actionlib package from ROS\cite{onlRosActionLib}. The actionlib stack provides a standardized interface for handling with highlevel task, so it is suitable for our use case. The module receives goal angles and sends the necessary commands to the ROS NXT Node (\ref{sec:nxt_node}) and provides continuous feedback and result information for this task.

\subsubsection{ROS Action Client}\label{sec:actionclient}
The Action Client is also built with the actionlib package from ROS. It sends the goal angles to the Action Server and handles the feedback and result information. 

\subsubsection{ROS NXT Control Node}\label{sec:nxt_node}
This module is responsible for the initial hardware setup of the NXT bricks. The used motors, sensors and their respective ports are specified in a configuration file and setup by this module. It also controls these hardware devices by sending steering commands to the device ports and it provides the current sensor states.

\section{Results}
It was shown that a 3-DOF arm built from LEGO Technic and controlled with a NXT is excellently accessible via a mobile app. In addition, the arm of 25x25x30cm has a portable size and can thus be well used for teaching experiments.\\
To test the repeatability of the joints, they were repeatedly moved 45 degrees from the starting position. \autoref{fig:lowerjointprecision} shows the positions that were approached with the lower joint. After 10 attempts, the accuracy is +/-0.5 degrees. In comparison, \autoref{fig:upperjointprecision} shows the positions approached by the upper joint. This shows an accuracy of +/-1.0 degrees. However, this is due to the lower translation.

\begin{figure}[htbp]
	\centerline{\includegraphics[scale=0.3]{img/angle_lower.png}}
	\caption{Angular positions of the lower arm after repeatedly triggering the same position.}
	\label{fig:lowerjointprecision}
\end{figure}

\begin{figure}[htbp]
	\centerline{\includegraphics[scale=0.3]{img/angle_upper.png}}
	\caption{Angular positions of the upper arm after repeatedly triggering the same position.}
	\label{fig:upperjointprecision}
\end{figure}

\section{Conclusion}
The NXT can be used to control the robot arm with a smartphone from any remote location. By using ROS this scenario can be extended quickly, e.g. by multiple users or a second arm. Despite the rather coarse LEGO Technic parts, the arm has a relatively high accuracy and is ideal for demonstrations or teaching. Anyway, the present scenario has several disadvantages, e.g. restrictions for cables and an additional computer.

\section{Further Work}
In further work, the NXT could be exchanged for a LEGO EV3 brick. On the one hand, the EV3 has one additional port to connect a sensor/device. On the other hand, it offers a WiFi interface, which eliminates the need for a laptop as connection supplier between smartphone and arm.
Currently the app offers only a 2D view of the arm. For a better operation, a 3D view could be developed.
Another important point would be to install more and better external sensors to achieve a higher positional accuracy of the arm.

\begin{thebibliography}{00}
\bibitem{onlROS} 
[Online] Available:
\\\texttt{http://wiki.ros.org/kinetic/Installation} [Accessed: 07-Jan-2019]
\bibitem{onlIEEEApp-Arm} 
K. Premkumar, K. Nigel ``Smart Phone Based Robotic Arm Control Using
Raspberry Pi, Android and Wi-Fi`` IEEE, March 2015
\bibitem{onlNXT-2DOF} 
[Online] Available:
\\\texttt{http://www.nxtprograms.com/robot\_arm/steps.html} [Accessed: 16-Jan-2019]
\bibitem{onlHackster} 
[Online] Available:
\\\texttt{https://www.hackster.io/slantconcepts/control-arduino-
	robot-arm-with-android-app-1c0d96} [Accessed: 14-Jan-2019]
\bibitem{onlLegoMindstorm} 
[Online] Available:
\\\texttt{https://www.lego.com/de-de/mindstorms/downloads/nxt-software-download} [Accessed: 10-Jan-2019]

\bibitem{onlRosActionLib} 
[Online] Available:
\\\texttt{https://wiki.ros.org/actionlib} [Accessed: 15-Jan-2019]


\end{thebibliography}

\end{document}